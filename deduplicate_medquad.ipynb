{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPd7y19UDFLRjhnp7mTK8CE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"17kwVJdUmxf-","executionInfo":{"status":"ok","timestamp":1735211296893,"user_tz":360,"elapsed":12499,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"outputs":[],"source":["# One run of test to deduplicate the bio_med_research dataset\n","import pandas as pd\n","import os\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","import json\n","from tqdm import tqdm\n","import pickle"]},{"cell_type":"code","source":["# if use colab, run this part\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/bionlp')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8VCKfw9m2u4","executionInfo":{"status":"ok","timestamp":1735211320730,"user_tz":360,"elapsed":23840,"user":{"displayName":"Bluey","userId":"09829445668619035203"}},"outputId":"dd1a0549-0e14-4d75-e1a6-e116f5aca559"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# go to model dir\n","os.chdir('MedImageInsights')"],"metadata":{"id":"F23EbDpGm4XV","executionInfo":{"status":"ok","timestamp":1735211322046,"user_tz":360,"elapsed":1319,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# set directory to deduplicate\n","directory = \"../deduplicated_data/self_medquad\""],"metadata":{"id":"Se8mJMvIm5wH","executionInfo":{"status":"ok","timestamp":1735211336250,"user_tz":360,"elapsed":1203,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# install necessary package\n","!pip install mup\n","!pip install fvcore"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0yEWi2-nBKV","executionInfo":{"status":"ok","timestamp":1735211345228,"user_tz":360,"elapsed":8422,"user":{"displayName":"Bluey","userId":"09829445668619035203"}},"outputId":"ee451d89-d827-441a-efa6-9b53d1f52dc1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mup\n","  Downloading mup-1.0.0.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mup) (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mup) (2.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mup) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from mup) (0.20.1+cu121)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from mup) (0.13.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mup) (4.67.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mup) (6.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->mup) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mup) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->mup) (2024.2)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->mup) (3.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mup) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mup) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mup) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mup) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mup) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->mup) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->mup) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->mup) (11.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->mup) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mup) (3.0.2)\n","Building wheels for collected packages: mup\n","  Building wheel for mup (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mup: filename=mup-1.0.0-py3-none-any.whl size=23629 sha256=d94b79c3879b21015225a72e93aaab5b209d0d9b5caa3fbe155eb12b1b66c549\n","  Stored in directory: /root/.cache/pip/wheels/f4/c8/88/3c23a3d10c50053b6552d2d30aee5b53ba89a47f742420036c\n","Successfully built mup\n","Installing collected packages: mup\n","Successfully installed mup-1.0.0\n","Collecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n","Collecting yacs>=0.1.6 (from fvcore)\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.67.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n","Collecting portalocker (from iopath>=0.1.7->fvcore)\n","  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=632b483ad7118296f4f98e9fc323c40b98ed7242a18b5113060773600831db58\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=13c28d1b51070a8aa49f9f109fbfb9d73b3882426faf2db915f689cf6b480dbe\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore\n","Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 yacs-0.1.8\n"]}]},{"cell_type":"code","source":["# load model\n","from medimageinsightmodel import MedImageInsight\n","\n","classifier = MedImageInsight(\n","    model_dir=\"2024.09.27\",\n","    vision_model_name=\"medimageinsigt-v1.0.0.pt\",\n","    language_model_name=\"language_model.pth\"\n",")\n","\n","classifier.load_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-Ft7QThnCjr","executionInfo":{"status":"ok","timestamp":1735211452817,"user_tz":360,"elapsed":107596,"user":{"displayName":"Bluey","userId":"09829445668619035203"}},"outputId":"17889b2b-ad13-4d05-b317-0aced05eba27"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded successfully on device: cuda\n"]}]},{"cell_type":"code","source":["all_csv_files = []\n","for root, dirs, files in os.walk(directory):\n","    for file in files:\n","        if file.endswith(\".csv\"):\n","            file_path = os.path.join(root, file)\n","            all_csv_files.append(file_path)"],"metadata":{"id":"r71QWwgmnKF_","executionInfo":{"status":"ok","timestamp":1735211476910,"user_tz":360,"elapsed":24115,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["data = {}\n","for f in all_csv_files:\n","    data[f] = pd.read_csv(f)"],"metadata":{"id":"3kPVgB8DnVDh","executionInfo":{"status":"ok","timestamp":1735218511663,"user_tz":360,"elapsed":7034770,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# deduplicate across all dataset"],"metadata":{"id":"v-EUx3WHnfFt"}},{"cell_type":"code","source":["# functions for deduplication\n","def get_embeddings(texts, batch_size = 64):\n","    embeddings = []\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i+batch_size]\n","        embeddings.extend(classifier.encode(texts = batch_texts)['text_embeddings'])\n","    return np.array(embeddings)\n","\n","def compute_similarity(embeddings, threshold = 0.9):\n","    # n = len(embeddings)\n","    # to_remove = set()\n","    # for i in tqdm(range(n), desc = \"Computing similarity\"):\n","    #     for j in range(i+1, n):\n","    #         sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))[0][0]\n","    #         if sim > threshold:\n","    #             to_remove.add(j)\n","    # return to_remove\n","    similarity_matrix = cosine_similarity(embeddings)\n","    np.fill_diagonal(similarity_matrix, 0)  # Ignore self-similarity\n","\n","    # Find indices of pairs with similarity above the threshold\n","    to_remove = set()\n","    for i in range(similarity_matrix.shape[0]):\n","        if i in to_remove:\n","            continue\n","        similar_indices = np.where(similarity_matrix[i] > threshold)[0]\n","        to_remove.update(similar_indices)\n","\n","    return to_remove\n","\n","def compute_similarity_chunked(embeddings, threshold=0.9, chunk_size=8000):\n","    \"\"\"\n","    Compute cosine similarity in chunks to reduce memory usage.\n","    \"\"\"\n","    n = len(embeddings)\n","    to_remove = set()\n","    for i in range(0, n, chunk_size):\n","        # Get the current chunk\n","        chunk_embeddings = embeddings[i:i + chunk_size]\n","\n","        # Compute cosine similarity for the current chunk against all embeddings\n","        similarity_matrix = cosine_similarity(chunk_embeddings, embeddings)\n","\n","        # Iterate through the chunk rows to find high-similarity indices\n","        for row_idx, similarities in enumerate(similarity_matrix):\n","            actual_idx = i + row_idx  # Map back to the original index\n","            if actual_idx in to_remove:\n","                continue\n","\n","            similar_indices = np.where(similarities > threshold)[0]\n","            similar_indices = [idx for idx in similar_indices if idx > actual_idx]  # Avoid duplicates\n","            to_remove.update(similar_indices)\n","\n","    return to_remove\n","\n","def compute_similarity_between_datasets(embeddings1, embeddings2, threshold = 0.9):\n","    to_remove = set()\n","    for i in range(len(embeddings1)):\n","        for j in range(len(embeddings2)):\n","            sim = cosine_similarity(embeddings1[i].reshape(1, -1), embeddings2[j].reshape(1, -1))[0][0]\n","            if sim > threshold:\n","                to_remove.add(j)\n","    return to_remove\n","\n","def compute_similarity_between_datasets_chunked(embeddings1, embeddings2, threshold=0.9, chunk_size1=8000, chunk_size2=8000):\n","    \"\"\"\n","    Compute cosine similarity between two datasets in chunks to reduce memory usage.\n","    Removes entries from embeddings1 based on high similarity with embeddings2.\n","    \"\"\"\n","    to_remove = set()\n","    n1, n2 = len(embeddings1), len(embeddings2)\n","\n","    for i in ange(0, n1, chunk_size1):\n","        # Get a chunk from embeddings1\n","        chunk_embeddings1 = embeddings1[i:i + chunk_size1]\n","\n","        for j in range(0, n2, chunk_size2):\n","            # Get a chunk from embeddings2\n","            chunk_embeddings2 = embeddings2[j:j + chunk_size2]\n","\n","            # Compute cosine similarity for the two chunks\n","            similarity_matrix = cosine_similarity(chunk_embeddings1, chunk_embeddings2)\n","\n","            # Check rows in chunk_embeddings1 with high similarity to chunk_embeddings2\n","            for row_idx, similarities in enumerate(similarity_matrix):\n","                actual_idx = i + row_idx  # Map back to the original index in embeddings1\n","                if actual_idx in to_remove:\n","                    continue\n","                if np.any(similarities > threshold):\n","                    to_remove.add(actual_idx)\n","\n","    return to_remove\n","\n","def deduplication_within_dataset_qa(dataset, threshold = 0.9):\n","    questions = dataset[\"question\"].tolist()\n","    #answers = dataset[\"answer\"].tolist()\n","\n","    question_embeddings = get_embeddings(questions)\n","    to_remove_questions = compute_similarity_chunked(question_embeddings, threshold)\n","\n","    new_dataset = dataset.drop(index = list(to_remove_questions)).reset_index(drop=True)\n","\n","    answers = new_dataset[\"answer\"].tolist()\n","    answer_embeddings = get_embeddings(answers)\n","    to_remove_answers = compute_similarity_chunked(answer_embeddings, threshold)\n","\n","    new_dataset = new_dataset.drop(index = list(to_remove_answers)).reset_index(drop=True)\n","    return new_dataset, list(to_remove_questions), list(to_remove_answers)\n","\n","\n","def deduplicate_across_datasets_qa(new_dataset, old_question_embeddings_saved, old_answer_embeddings_saved, threshold = 0.9):\n","    # Combine all old dataset questions and answers\n","    # all_old_questions = []\n","    # all_old_answers = []\n","\n","    # for dataset in old_datasets:\n","    #     all_old_questions.extend(dataset[\"question\"].tolist())\n","    #     all_old_answers.extend(dataset[\"answer\"].tolist())\n","\n","    # Generate embeddings for old dataset questions and answers\n","    # old_question_embeddings = get_embeddings(all_old_questions)\n","    # old_answer_embeddings = get_embeddings(all_old_answers)\n","    old_question_embeddings = []\n","    old_answer_embeddings = []\n","    for old_embed in old_question_embeddings_saved:\n","        old_question_embeddings.extend(old_embed)\n","    for old_embed in old_answer_embeddings_saved:\n","        old_answer_embeddings.extend(old_embed)\n","\n","    # Generate embeddings for new dataset questions and answers\n","    new_question_embeddings = get_embeddings(new_dataset[\"question\"].tolist())\n","    new_answer_embeddings = get_embeddings(new_dataset[\"answer\"].tolist())\n","\n","    # Deduplicate new questions\n","    to_remove_questions = compute_similarity_between_datasets_chunked(new_question_embeddings, old_question_embeddings)\n","\n","    # Deduplicate new answers\n","    to_remove_answers = compute_similarity_between_datasets_chunked(new_answer_embeddings, old_answer_embeddings)\n","\n","    # Combine removal indices\n","    to_remove = to_remove_questions.union(to_remove_answers)\n","\n","    # Drop duplicates from new dataset\n","    deduplicated_new_dataset = new_dataset.drop(index=list(to_remove)).reset_index(drop=True)\n","\n","    return deduplicated_new_dataset, list(to_remove_questions), list(to_remove_answers)\n","\n"],"metadata":{"id":"ShY3YoZboyFf","executionInfo":{"status":"ok","timestamp":1735218511664,"user_tz":360,"elapsed":8,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["old_questions = []\n","old_answers = []\n","\n","with open(\"../deduplicated_embeddings/QAs/medicationqa_question_embeddings.pkl\", \"rb\") as f:\n","    medication_qa_q_embed = pickle.load(f)\n","    old_questions.append(medication_qa_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medicationqa_answer_embeddings.pkl\", \"rb\") as f:\n","    medication_qa_a_embed = pickle.load(f)\n","    old_answers.append(medication_qa_a_embed)\n","\n","#pubmed1,2,3\n","with open(\"../deduplicated_embeddings/QAs/pubmed1_question_embeddings.pkl\", \"rb\") as f:\n","    pubmed1_q_embed = pickle.load(f)\n","    old_questions.append(pubmed1_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/pubmed1_answer_embeddings.pkl\", \"rb\") as f:\n","    pubmed1_a_embed = pickle.load(f)\n","    old_answers.append(pubmed1_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/pubmed2_question_embeddings.pkl\", \"rb\") as f:\n","    pubmed2_q_embed = pickle.load(f)\n","    old_questions.append(pubmed2_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/pubmed2_answer_embeddings.pkl\", \"rb\") as f:\n","    pubmed2_a_embed = pickle.load(f)\n","    old_answers.append(pubmed2_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/pubmed3_question_embeddings.pkl\", \"rb\") as f:\n","    pubmed3_q_embed = pickle.load(f)\n","    old_questions.append(pubmed3_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/pubmed3_answer_embeddings.pkl\", \"rb\") as f:\n","    pubmed3_a_embed = pickle.load(f)\n","    old_answers.append(pubmed3_a_embed)\n","\n","# medmcqa\n","with open(\"../deduplicated_embeddings/QAs/medmcqa_train_question_embeddings.pkl\", \"rb\") as f:\n","    medmcqa_train_q_embed = pickle.load(f)\n","    old_questions.append(medmcqa_train_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medmcqa_train_answer_embeddings.pkl\", \"rb\") as f:\n","    medmcqa_train_a_embed = pickle.load(f)\n","    old_answers.append(medmcqa_train_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medmcqa_dev_question_embeddings.pkl\", \"rb\") as f:\n","    medmcqa_dev_q_embed = pickle.load(f)\n","    old_questions.append(medmcqa_dev_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medmcqa_dev_answer_embeddings.pkl\", \"rb\") as f:\n","    medmcqa_dev_a_embed = pickle.load(f)\n","    old_answers.append(medmcqa_dev_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medmcqa_test_question_embeddings.pkl\", \"rb\") as f:\n","    medmcqa_test_q_embed = pickle.load(f)\n","    old_questions.append(medmcqa_test_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medmcqa_test_answer_embeddings.pkl\", \"rb\") as f:\n","    medmcqa_test_a_embed = pickle.load(f)\n","    old_answers.append(medmcqa_test_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medqa_train_question_embeddings.pkl\", \"rb\") as f:\n","    medqa_train_a_embed = pickle.load(f)\n","    old_questions.append(medqa_train_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medqa_train_answer_embeddings.pkl\", \"rb\") as f:\n","    medqa_train_a_embed = pickle.load(f)\n","    old_answers.append(medqa_train_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medqa_dev_question_embeddings.pkl\", \"rb\") as f:\n","    medqa_dev_a_embed = pickle.load(f)\n","    old_questions.append(medqa_dev_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medqa_dev_answer_embeddings.pkl\", \"rb\") as f:\n","    medqa_dev_a_embed = pickle.load(f)\n","    old_answers.append(medqa_dev_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medqa_test_question_embeddings.pkl\", \"rb\") as f:\n","    medqa_test_a_embed = pickle.load(f)\n","    old_questions.append(medqa_test_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/medqa_test_answer_embeddings.pkl\", \"rb\") as f:\n","    medqa_test_a_embed = pickle.load(f)\n","    old_answers.append(medqa_test_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/trec_train1_question_embeddings.pkl\", \"rb\") as f:\n","    trec_train1_q_embed = pickle.load(f)\n","    old_questions.append(trec_train1_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/trec_train1_answer_embeddings.pkl\", \"rb\") as f:\n","    trec_train1_a_embed = pickle.load(f)\n","    old_answers.append(trec_train1_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/trec_train2_question_embeddings.pkl\", \"rb\") as f:\n","    trec_train2_q_embed = pickle.load(f)\n","    old_questions.append(trec_train2_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/trec_train2_answer_embeddings.pkl\", \"rb\") as f:\n","    trec_train2_a_embed = pickle.load(f)\n","    old_answers.append(trec_train2_a_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/trec_test_question_embeddings.pkl\", \"rb\") as f:\n","    trec_test_q_embed = pickle.load(f)\n","    old_questions.append(trec_test_q_embed)\n","\n","with open(\"../deduplicated_embeddings/QAs/trec_test_answer_embeddings.pkl\", \"rb\") as f:\n","    trec_test_a_embed = pickle.load(f)\n","    old_answers.append(trec_test_a_embed)"],"metadata":{"id":"Jb1U7nm6njd_","executionInfo":{"status":"ok","timestamp":1735218621061,"user_tz":360,"elapsed":109403,"user":{"displayName":"Bluey","userId":"09829445668619035203"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# remove na for all data\n","def clean_dataframe(df):\n","    # Ensure \"question\" and \"answer\" columns exist and are non-empty\n","    df[\"question\"] = df[\"question\"].fillna(\"\").astype(str)\n","    df[\"answer\"] = df[\"answer\"].fillna(\"\").astype(str)\n","\n","    # Remove rows where \"question\" or \"answer\" is an empty string\n","    df = df[(df[\"question\"].str.strip() != \"\") & (df[\"answer\"].str.strip() != \"\")]\n","    return df.reset_index(drop=True)\n","\n","cleaned_data = {}\n","for k in tqdm(data,desc = \"Cleaning data\"):\n","    cleaned_data[k] = clean_dataframe(data[k])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RB2rFQQJGInn","executionInfo":{"status":"ok","timestamp":1735218861759,"user_tz":360,"elapsed":22659,"user":{"displayName":"Bluey","userId":"09829445668619035203"}},"outputId":"ac025277-bdc3-4c40-fc54-77ec33cbdf5b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Cleaning data: 100%|██████████| 11268/11268 [00:21<00:00, 519.08it/s]\n"]}]},{"cell_type":"code","source":["deduplicated_dict = {}\n","\n","for k in tqdm(data, desc = \"Deduplicating\"):\n","    full_deduplicated_dataset, q_to_remove, a_to_remove = deduplication_within_dataset_qa(data[k])\n","    deduplicated_dict[k] = full_deduplicated_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqI6uu37ooUN","executionInfo":{"status":"ok","timestamp":1735219411169,"user_tz":360,"elapsed":545353,"user":{"displayName":"Bluey","userId":"09829445668619035203"}},"outputId":"11c142f4-e060-4ce3-8202-e6087a880642"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Deduplicating: 100%|██████████| 11268/11268 [09:04<00:00, 20.69it/s]\n"]}]},{"cell_type":"code","source":["# save data\n","for k in tqdm(deduplicated_dict, desc = \"Saving data\"):\n","    subdir = k.split('/')[-2]\n","    if not os.path.exists(os.path.join(\"../deduplicated_data/MedQuAD\", subdir)):\n","        os.makedirs(os.path.join(\"../deduplicated_data/MedQuAD\", subdir))\n","    deduplicated_dict[k].to_csv(os.path.join(\"../deduplicated_data/MedQuAD\", subdir, k.split('/')[-1]), index = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rYtIuN3KpROM","executionInfo":{"status":"ok","timestamp":1735219740109,"user_tz":360,"elapsed":66397,"user":{"displayName":"Bluey","userId":"09829445668619035203"}},"outputId":"2f1e7957-c757-483b-b6f6-3d603c7cf43e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving data: 100%|██████████| 11268/11268 [01:06<00:00, 170.43it/s]\n"]}]}]}