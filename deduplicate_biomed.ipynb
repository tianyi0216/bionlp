{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One run of test to deduplicate the bio_med_research dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if use colab, run this part\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/bionlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to model dir\n",
    "os.chdir('MedImageInsights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory to deduplicate\n",
    "directory = \"../dataset/bio_med_research\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mup in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (1.24.4)\n",
      "Requirement already satisfied: pandas in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (1.5.3)\n",
      "Requirement already satisfied: torch in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (2.6.0.dev20240927)\n",
      "Requirement already satisfied: torchvision in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (0.20.0.dev20240928)\n",
      "Requirement already satisfied: seaborn in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (0.12.2)\n",
      "Requirement already satisfied: tqdm in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (4.67.1)\n",
      "Requirement already satisfied: pyyaml in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from mup) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from pandas->mup) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from pandas->mup) (2023.3.post1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from seaborn->mup) (3.7.2)\n",
      "Requirement already satisfied: filelock in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torch->mup) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torch->mup) (4.11.0)\n",
      "Requirement already satisfied: networkx in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torch->mup) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torch->mup) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torch->mup) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torch->mup) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch->mup) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from torchvision->mup) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->mup) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->mup) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->mup) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->mup) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->mup) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->mup) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->mup) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from jinja2->torch->mup) (2.1.1)\n",
      "Requirement already satisfied: fvcore in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: numpy in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (1.24.4)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (6.0)\n",
      "Requirement already satisfied: tqdm in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (2.4.0)\n",
      "Requirement already satisfied: Pillow in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (9.4.0)\n",
      "Requirement already satisfied: tabulate in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (0.8.10)\n",
      "Requirement already satisfied: iopath>=0.1.7 in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from fvcore) (0.1.10)\n",
      "Requirement already satisfied: typing-extensions in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from iopath>=0.1.7->fvcore) (4.11.0)\n",
      "Requirement already satisfied: portalocker in /Users/tianyixu/anaconda3/lib/python3.11/site-packages (from iopath>=0.1.7->fvcore) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "# install necessary package\n",
    "!pip install mup\n",
    "!pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyixu/anaconda3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from medimageinsightmodel import MedImageInsight\n",
    "\n",
    "classifier = MedImageInsight(\n",
    "    model_dir=\"2024.09.27\",\n",
    "    vision_model_name=\"medimageinsigt-v1.0.0.pt\",\n",
    "    language_model_name=\"language_model.pth\"\n",
    ")\n",
    "\n",
    "classifier.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "def parse_xml(file):\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    sentence_data = []\n",
    "    for sentence in root.findall('sentence'):\n",
    "        sentence_id = sentence.get('id')\n",
    "        sentence_text = sentence.get('text')\n",
    "\n",
    "        sentence_data.append({\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"sentence_text\": sentence_text\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(sentence_data)\n",
    "\n",
    "\n",
    "def load_dataset(path, filetype = \"csv\"):\n",
    "    if filetype == \"csv\":\n",
    "        all_files = []\n",
    "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading CSV files\"):\n",
    "            for file in tqdm(files, desc = \"Processing file\"):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        ds = {}\n",
    "        for f in all_files:\n",
    "            df = pd.read_csv(f)\n",
    "            ds[f] = df\n",
    "        return ds\n",
    "    elif filetype == \"xml\":\n",
    "        all_files = []\n",
    "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading XML files\"):\n",
    "            for file in tqdm(files, desc = \"Processing file\"):\n",
    "                if file.endswith(\".xml\"):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        ds = {}\n",
    "        for f in all_files:\n",
    "            ds[f] = parse_xml(f)\n",
    "        return ds\n",
    "    elif filetype == \"jsonl\":\n",
    "        all_files = []\n",
    "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading JSONL files\"):\n",
    "            for file in tqdm(files, desc = \"Processing file\"):\n",
    "                if file.endswith(\".jsonl\"):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        ds = {}\n",
    "        for f in all_files:\n",
    "            print(\"current file: \", f)\n",
    "            with open(f, \"r\") as file:\n",
    "                data = [json.loads(line) for line in file]\n",
    "            ds[f] = pd.DataFrame(data)\n",
    "        return ds\n",
    "    elif filetype == \"json\":\n",
    "        all_files = []\n",
    "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading JSON files\"):\n",
    "            for file in tqdm(files, desc = \"Processing file\"):\n",
    "                if file.endswith(\".json\"):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        ds = {}\n",
    "        for f in all_files:\n",
    "            with open(f, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            ds[f] = pd.DataFrame(data)\n",
    "        return ds\n",
    "    elif filetype == \"txt\":\n",
    "        all_files = []\n",
    "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading TXT files\"):\n",
    "            for file in tqdm(files, desc = \"Processing file\"):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        print(all_files)\n",
    "        ds = {}\n",
    "        for f in all_files:\n",
    "            with open(f, \"r\") as file:\n",
    "                data = file.readlines()\n",
    "            ds[f] = data\n",
    "        return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file: 100%|██████████| 6/6 [00:00<00:00, 35494.82it/s]\n",
      "Loading TXT files: 1it [00:00, 482.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/bio_med_research/bc5cdr/val_bc5cdr.txt', '../dataset/bio_med_research/bc5cdr/test_bc5cdr.txt', '../dataset/bio_med_research/bc5cdr/train_bc5cdr.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bc5cdr = load_dataset(directory + \"/bc5cdr\", \"txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc5cdr_train = bc5cdr['../dataset/bio_med_research/bc5cdr/train_bc5cdr.txt']\n",
    "bc5cdr_val = bc5cdr['../dataset/bio_med_research/bc5cdr/val_bc5cdr.txt']\n",
    "bc5cdr_test = bc5cdr['../dataset/bio_med_research/bc5cdr/test_bc5cdr.txt']\n",
    "\n",
    "bc5cdr_train_df = pd.DataFrame()\n",
    "bc5cdr_val_df = pd.DataFrame()\n",
    "bc5cdr_test_df = pd.DataFrame()\n",
    "\n",
    "bc5cdr_train_df[\"text\"] = None\n",
    "bc5cdr_val_df[\"text\"] = None\n",
    "bc5cdr_test_df[\"text\"] = None\n",
    "\n",
    "for i in range(len(bc5cdr_train)):\n",
    "    bc5cdr_train_df.at[i, \"text\"] = bc5cdr_train[i]\n",
    "    bc5cdr_val_df.at[i, \"text\"] = bc5cdr_val[i]\n",
    "    bc5cdr_test_df.at[i, \"text\"] = bc5cdr_test[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column information\n",
    "col_info = pd.read_csv(\"../col.csv\", quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>column_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc5cdr</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BioNLI</td>\n",
       "      <td>supp_set, conclusion, label_cat, ori_conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CORD19</td>\n",
       "      <td>title, abstract, full_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDCICorpus</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hoc</td>\n",
       "      <td>text, label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pubmed</td>\n",
       "      <td>MedlineCitation, PubmedData</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SourceData</td>\n",
       "      <td>words,labels,tag_mask,text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trec_covid</td>\n",
       "      <td>title, text, metadata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name                                      column_name\n",
       "0       bc5cdr                                             text\n",
       "1       BioNLI  supp_set, conclusion, label_cat, ori_conclusion\n",
       "2       CORD19                       title, abstract, full_text\n",
       "3   DDCICorpus                                         sentence\n",
       "4          hoc                                      text, label\n",
       "5       pubmed                      MedlineCitation, PubmedData\n",
       "6   SourceData                       words,labels,tag_mask,text\n",
       "7   trec_covid                            title, text, metadata"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate the dataset\n",
    "def get_embeddings(texts, batch_size = 64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc = \"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        embeddings.extend(classifier.encode(texts = batch_texts)['text_embeddings'])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def compute_similarity_chunked(embeddings, threshold=0.9, chunk_size=8000):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity in chunks to reduce memory usage.\n",
    "    \"\"\"\n",
    "    n = len(embeddings)\n",
    "    to_remove = set()\n",
    "    for i in tqdm(range(0, n, chunk_size), desc= \"Calcuating Similarity\"):\n",
    "        # Get the current chunk\n",
    "        chunk_embeddings = embeddings[i:i + chunk_size]\n",
    "\n",
    "        # Compute cosine similarity for the current chunk against all embeddings\n",
    "        similarity_matrix = cosine_similarity(chunk_embeddings, embeddings)\n",
    "\n",
    "        # Iterate through the chunk rows to find high-similarity indices\n",
    "        for row_idx, similarities in enumerate(similarity_matrix):\n",
    "            actual_idx = i + row_idx  # Map back to the original index\n",
    "            if actual_idx in to_remove:\n",
    "                continue\n",
    "\n",
    "            similar_indices = np.where(similarities > threshold)[0]\n",
    "            similar_indices = [idx for idx in similar_indices if idx > actual_idx]  # Avoid duplicates\n",
    "            to_remove.update(similar_indices)\n",
    "\n",
    "    return to_remove\n",
    "\n",
    "def compute_similarity_between_datasets_chunked(embeddings1, embeddings2, threshold=0.9, chunk_size1=8000, chunk_size2=8000):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two datasets in chunks to reduce memory usage.\n",
    "    Removes entries from embeddings1 based on high similarity with embeddings2.\n",
    "    \"\"\"\n",
    "    to_remove = set()\n",
    "    n1, n2 = len(embeddings1), len(embeddings2)\n",
    "\n",
    "    for i in tqdm(range(0, n1, chunk_size1), desc=\"Processing dataset1 in chunks\"):\n",
    "        # Get a chunk from embeddings1\n",
    "        chunk_embeddings1 = embeddings1[i:i + chunk_size1]\n",
    "\n",
    "        for j in range(0, n2, chunk_size2):\n",
    "            # Get a chunk from embeddings2\n",
    "            chunk_embeddings2 = embeddings2[j:j + chunk_size2]\n",
    "\n",
    "            # Compute cosine similarity for the two chunks\n",
    "            similarity_matrix = cosine_similarity(chunk_embeddings1, chunk_embeddings2)\n",
    "\n",
    "            # Check rows in chunk_embeddings1 with high similarity to chunk_embeddings2\n",
    "            for row_idx, similarities in enumerate(similarity_matrix):\n",
    "                actual_idx = i + row_idx  # Map back to the original index in embeddings1\n",
    "                if actual_idx in to_remove:\n",
    "                    continue\n",
    "                if np.any(similarities > threshold):\n",
    "                    to_remove.add(actual_idx)\n",
    "\n",
    "    return to_remove\n",
    "\n",
    "def deduplicate_within_dataset(dataset, columns,threshold=0.9):\n",
    "    # joins the columns in the dataset\n",
    "    texts = list(dataset[columns].apply(lambda x: \" \".join(x.values.astype(str)), axis=1))\n",
    "    embeddings = get_embeddings(texts)\n",
    "    to_remove = compute_similarity_chunked(embeddings, threshold=threshold)\n",
    "    number_removed = len(to_remove)\n",
    "    return dataset.drop(to_remove), number_removed\n",
    "\n",
    "def deduplicate_between_datasets(new_dataset, columns, old_embeddings, threshold=0.9):\n",
    "    texts1 = list(new_dataset[columns].apply(lambda x: \" \".join(x.values.astype(str)), axis=1))\n",
    "    embeddings1 = get_embeddings(texts1)\n",
    "    old_embeddings_list = []\n",
    "    for embed in old_embeddings:\n",
    "        old_embeddings_list.extend(embed)\n",
    "    to_remove = compute_similarity_between_datasets_chunked(embeddings1, old_embeddings_list, threshold=threshold)\n",
    "    number_removed = len(to_remove)\n",
    "    return new_dataset.drop(to_remove), number_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate within dataset\n",
    "deduplicated_bc5cdr_train, number_removed_train = deduplicate_within_dataset(bc5cdr_train_df, col_info.loc[col_info[\"dataset_name\"] == \"bc5cdr\", \"column_name\"].tolist())\n",
    "deduplicated_bc5cdr_val, number_removed_val = deduplicate_within_dataset(bc5cdr_val_df, col_info.loc[col_info[\"dataset_name\"] == \"bc5cdr\", \"column_name\"].tolist())\n",
    "deduplicated_bc5cdr_test, number_removed_test = deduplicate_within_dataset(bc5cdr_test_df, col_info.loc[col_info[\"dataset_name\"] == \"bc5cdr\", \"column_name\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Number of removed samples in train: \", number_removed_train)\n",
    "print(\"Number of removed samples in val: \", number_removed_val)\n",
    "print(\"Number of removed samples in test: \", number_removed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicated_train will serve as the base ds and we add the other datasets to it\n",
    "deduplicated_bc5cdr_train.to_csv(\"../deduplicated_data/bio_med_research/bc5cdr/train_bc5cdr_deduplicated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old datasets\n",
    "old_datas = []\n",
    "\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bc5cdr_train_embeddings.pkl\", \"rb\") as f:\n",
    "    bc5cdr_train_embeddings = pickle.load(f)\n",
    "    old_datas.append(bc5cdr_train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate between existing dataset\n",
    "full_deduplicate_bc5cdr_val, removed_idx_val = deduplicate_between_datasets(deduplicated_bc5cdr_val, [\"text\"], old_datas)\n",
    "print(removed_idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the deduplicated dataset\n",
    "full_deduplicate_bc5cdr_val.to_csv(\"../deduplicated_data/bio_med_research/bc5cdr/val_bc5cdr_deduplicated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bc5cdr_val_embeddings.pkl\", \"rb\") as f:\n",
    "    bc5cdr_val_embeddings = pickle.load(f)\n",
    "    old_datas.append(bc5cdr_val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate between existing dataset\n",
    "full_deduplicate_bc5cdr_test, removed_idx_test = deduplicate_between_datasets(deduplicated_bc5cdr_test, [\"text\"], old_datas)\n",
    "print(removed_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the deduplicated dataset\n",
    "full_deduplicate_bc5cdr_test.to_csv(\"../deduplicated_data/bio_med_research/bc5cdr/test_bc5cdr_deduplicated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate bionli\n",
    "bionli = load_dataset(directory + \"/BioNLI\", \"csv\")\n",
    "bionli_train = bionli['../dataset/bio_med_research/BioNLI/train_balanced.csv']\n",
    "bionli_dev = bionli['../dataset/bio_med_research/BioNLI/dev_balanced.csv']\n",
    "bionli_test = bionli['../dataset/bio_med_research/BioNLI/test.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate within dataset\n",
    "deduplicated_bionli_train, number_removed_train = deduplicate_within_dataset(bionli_train, col_info.loc[col_info[\"dataset_name\"] == \"BioNLI\", \"column_name\"].tolist())\n",
    "deduplicated_bionli_dev, number_removed_dev = deduplicate_within_dataset(bionli_dev, col_info.loc[col_info[\"dataset_name\"] == \"BioNLI\", \"column_name\"].tolist())\n",
    "deduplicated_bionli_test, number_removed_test = deduplicate_within_dataset(bionli_test, col_info.loc[col_info[\"dataset_name\"] == \"BioNLI\", \"column_name\"].tolist())\n",
    "\n",
    "print(\"Number of removed samples in train: \", number_removed_train)\n",
    "print(\"Number of removed samples in dev: \", number_removed_dev)\n",
    "print(\"Number of removed samples in test: \", number_removed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_datasets\n",
    "old_datas = []\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bc5cdr_train_embeddings.pkl\", \"rb\") as f:\n",
    "    bc5cdr_train_embeddings = pickle.load(f)\n",
    "    old_datas.append(bc5cdr_train_embeddings)\n",
    "\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bc5cdr_val_embeddings.pkl\", \"rb\") as f:\n",
    "    bc5cdr_val_embeddings = pickle.load(f)\n",
    "    old_datas.append(bc5cdr_val_embeddings)\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bc5cdr_test_embeddings.pkl\", \"rb\") as f:\n",
    "    bc5cdr_test_embeddings = pickle.load(f)\n",
    "    old_datas.append(bc5cdr_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate between existing dataset\n",
    "full_deduplicate_bionli_train, removed_idx_train = deduplicate_between_datasets(deduplicated_bionli_train, col_info.loc[col_info[\"dataset_name\"] == \"bionli\", \"column_name\"].tolist(), old_datas)\n",
    "print(removed_idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the deduplicated dataset\n",
    "full_deduplicate_bionli_train.to_csv(\"../deduplicated_data/bio_med_research/bionli/train_bionli_deduplicated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new dataset\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bionli_train_embeddings.pkl\", \"rb\") as f:\n",
    "    bionli_train_embeddings = pickle.load(f)\n",
    "    old_datas.append(bionli_train_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_deduplicate_bionli_val, removed_idx_val = deduplicate_between_datasets(deduplicated_bionli_val, col_info.loc[col_info[\"dataset_name\"] == \"bionli\", \"column_name\"].tolist(), old_datas)\n",
    "print(removed_idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the deduplicated dataset\n",
    "full_deduplicate_bionli_val.to_csv(\"../deduplicated_data/bio_med_research/bionli/val_bionli_deduplicated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new dataset\n",
    "with open(\"../deduplicated_embeddings/bio_med_research/bionli_val_embeddings.pkl\", \"rb\") as f:\n",
    "    bionli_val_embeddings = pickle.load(f)\n",
    "    old_datas.append(bionli_val_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_deduplicate_bionli_test, removed_idx_test = deduplicate_between_datasets(deduplicated_bionli_test, col_info.loc[col_info[\"dataset_name\"] == \"bionli\", \"column_name\"].tolist(), old_datas)\n",
    "print(removed_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the deduplicated dataset\n",
    "full_deduplicate_bionli_test.to_csv(\"../deduplicated_data/bio_med_research/bionli/test_bionli_deduplicated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load the dataset from huggingface first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6 = load_dataset(\"bigbio/bc5cdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = dataset6['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'document_id': '227508', 'type': 'title', 'text': 'Naloxone reverses the antihypertensive effect of clonidine.', 'entities': [{'id': '0', 'offsets': [[0, 8]], 'text': ['Naloxone'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D009270'}]}, {'id': '1', 'offsets': [[49, 58]], 'text': ['clonidine'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D003000'}]}], 'relations': [{'id': 'R0', 'type': 'CID', 'arg1_id': 'D008750', 'arg2_id': 'D007022'}]}, {'document_id': '227508', 'type': 'abstract', 'text': 'In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhibited or reversed by nalozone, 0.2 to 2 mg/kg. The hypotensive effect of 100 mg/kg alpha-methyldopa was also partially reversed by naloxone. Naloxone alone did not affect either blood pressure or heart rate. In brain membranes from spontaneously hypertensive rats clonidine, 10(-8) to 10(-5) M, did not influence stereoselective binding of [3H]-naloxone (8 nM), and naloxone, 10(-8) to 10(-4) M, did not influence clonidine-suppressible binding of [3H]-dihydroergocryptine (1 nM). These findings indicate that in spontaneously hypertensive rats the effects of central alpha-adrenoceptor stimulation involve activation of opiate receptors. As naloxone and clonidine do not appear to interact with the same receptor site, the observed functional antagonism suggests the release of an endogenous opiate by clonidine or alpha-methyldopa and the possible role of the opiate in the central control of sympathetic tone.', 'entities': [{'id': '2', 'offsets': [[93, 105]], 'text': ['hypertensive'], 'type': 'Disease', 'normalized': [{'db_name': 'MESH', 'db_id': 'D006973'}]}, {'id': '3', 'offsets': [[181, 190]], 'text': ['clonidine'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D003000'}]}, {'id': '4', 'offsets': [[244, 252]], 'text': ['nalozone'], 'type': 'Chemical', 'normalized': []}, {'id': '5', 'offsets': [[274, 285]], 'text': ['hypotensive'], 'type': 'Disease', 'normalized': [{'db_name': 'MESH', 'db_id': 'D007022'}]}, {'id': '6', 'offsets': [[306, 322]], 'text': ['alpha-methyldopa'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D008750'}]}, {'id': '7', 'offsets': [[354, 362]], 'text': ['naloxone'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D009270'}]}, {'id': '8', 'offsets': [[364, 372]], 'text': ['Naloxone'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D009270'}]}, {'id': '9', 'offsets': [[469, 481]], 'text': ['hypertensive'], 'type': 'Disease', 'normalized': [{'db_name': 'MESH', 'db_id': 'D006973'}]}, {'id': '10', 'offsets': [[487, 496]], 'text': ['clonidine'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D003000'}]}, {'id': '11', 'offsets': [[563, 576]], 'text': ['[3H]-naloxone'], 'type': 'Chemical', 'normalized': []}, {'id': '12', 'offsets': [[589, 597]], 'text': ['naloxone'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D009270'}]}, {'id': '13', 'offsets': [[637, 646]], 'text': ['clonidine'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D003000'}]}, {'id': '14', 'offsets': [[671, 695]], 'text': ['[3H]-dihydroergocryptine'], 'type': 'Chemical', 'normalized': []}, {'id': '15', 'offsets': [[750, 762]], 'text': ['hypertensive'], 'type': 'Disease', 'normalized': [{'db_name': 'MESH', 'db_id': 'D006973'}]}, {'id': '16', 'offsets': [[865, 873]], 'text': ['naloxone'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D009270'}]}, {'id': '17', 'offsets': [[878, 887]], 'text': ['clonidine'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D003000'}]}, {'id': '18', 'offsets': [[1026, 1035]], 'text': ['clonidine'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D003000'}]}, {'id': '19', 'offsets': [[1039, 1055]], 'text': ['alpha-methyldopa'], 'type': 'Chemical', 'normalized': [{'db_name': 'MESH', 'db_id': 'D008750'}]}], 'relations': [{'id': 'R0', 'type': 'CID', 'arg1_id': 'D008750', 'arg2_id': 'D007022'}]}]\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ds_test['passages'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.to_csv(\"example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the file \n",
    "for i in range(len(ds_test['passages'])):\n",
    "    with open(\"../dataset/bio_med_research/bc5cdr/train_bc5cdr.txt\", \"a\") as f:\n",
    "        f.write(str(ds_test['passages'][i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset6['test']['passages'])):\n",
    "    with open(\"../dataset/bio_med_research/bc5cdr/test_bc5cdr.txt\", \"a\") as f:\n",
    "        f.write(str(dataset6['test']['passages'][i]) + \"\\n\")\n",
    "for i in range(len(dataset6['validation']['passages'])):\n",
    "    with open(\"../dataset/bio_med_research/bc5cdr/val_bc5cdr.txt\", \"a\") as f:\n",
    "        f.write(str(dataset6['validation']['passages'][i]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
