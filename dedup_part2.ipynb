{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN4pl_ftqVue"
      },
      "outputs": [],
      "source": [
        "# One run of test to deduplicate the bio_med_research dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if use colab, run this part\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/bionlp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scExJgUWqYtZ",
        "outputId": "c4cb56a1-1757-4c46-cdc8-b2e7ee416d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# go to model dir\n",
        "os.chdir('MedImageInsights')"
      ],
      "metadata": {
        "id": "Yj-0uSzaqtus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set directory to deduplicate\n",
        "directory = \"../dataset/QAs\""
      ],
      "metadata": {
        "id": "HE2_qW2IrTQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary package\n",
        "!pip install mup\n",
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8lsM76XrWK-",
        "outputId": "342dd4d6-3206-40e3-f1b6-6ea2177ada67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mup\n",
            "  Downloading mup-1.0.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mup) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mup) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mup) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from mup) (0.20.1+cu121)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from mup) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mup) (4.66.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mup) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->mup) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mup) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->mup) (2024.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->mup) (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mup) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mup) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mup) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mup) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mup) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->mup) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->mup) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->mup) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->mup) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mup) (3.0.2)\n",
            "Building wheels for collected packages: mup\n",
            "  Building wheel for mup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mup: filename=mup-1.0.0-py3-none-any.whl size=23629 sha256=e173466e45ca07a301df9b7086aabecbc82b1dc184916beffcdfbee22a807040\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/88/3c23a3d10c50053b6552d2d30aee5b53ba89a47f742420036c\n",
            "Successfully built mup\n",
            "Installing collected packages: mup\n",
            "Successfully installed mup-1.0.0\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=afdf7eaf7fc6641a8cfb61000f2b174d28779e2e60c97f8b2ef83c7431713d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=3eed7341538f2b697f5be8a96b9fd906752bca1ea157fddf7121d31c7f177668\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "from medimageinsightmodel import MedImageInsight\n",
        "\n",
        "classifier = MedImageInsight(\n",
        "    model_dir=\"2024.09.27\",\n",
        "    vision_model_name=\"medimageinsigt-v1.0.0.pt\",\n",
        "    language_model_name=\"language_model.pth\"\n",
        ")\n",
        "\n",
        "classifier.load_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8wltqKzra3T",
        "outputId": "e5d45b7c-4dc5-49bd-98f2-ee9f6c74590e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "def parse_xml(file):\n",
        "    tree = ET.parse(file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    sentence_data = []\n",
        "    for sentence in root.findall('sentence'):\n",
        "        sentence_id = sentence.get('id')\n",
        "        sentence_text = sentence.get('text')\n",
        "\n",
        "        sentence_data.append({\n",
        "            \"sentence_id\": sentence_id,\n",
        "            \"sentence_text\": sentence_text\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(sentence_data)\n",
        "\n",
        "\n",
        "def load_dataset(path, filetype = \"csv\"):\n",
        "    if filetype == \"csv\":\n",
        "        all_files = []\n",
        "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading CSV files\"):\n",
        "            for file in tqdm(files, desc = \"Processing file\"):\n",
        "                if file.endswith(\".csv\"):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "        ds = {}\n",
        "        for f in all_files:\n",
        "            df = pd.read_csv(f)\n",
        "            ds[f] = df\n",
        "        return ds\n",
        "    elif filetype == \"xml\":\n",
        "        all_files = []\n",
        "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading XML files\"):\n",
        "            for file in tqdm(files, desc = \"Processing file\"):\n",
        "                if file.endswith(\".xml\"):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "        ds = {}\n",
        "        for f in all_files:\n",
        "            ds[f] = parse_xml(f)\n",
        "        return ds\n",
        "    elif filetype == \"jsonl\":\n",
        "        all_files = []\n",
        "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading JSONL files\"):\n",
        "            for file in tqdm(files, desc = \"Processing file\"):\n",
        "                if file.endswith(\".jsonl\"):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "        ds = {}\n",
        "        for f in all_files:\n",
        "            print(\"current file: \", f)\n",
        "            with open(f, \"r\") as file:\n",
        "                data = [json.loads(line) for line in file]\n",
        "            ds[f] = pd.DataFrame(data)\n",
        "        return ds\n",
        "    elif filetype == \"json\":\n",
        "        all_files = []\n",
        "        for root, dirs, files in tqdm(os.walk(path), desc = \"Loading JSON files\"):\n",
        "            for file in tqdm(files, desc = \"Processing file\"):\n",
        "                if file.endswith(\".json\"):\n",
        "                    all_files.append(os.path.join(root, file))\n",
        "        ds = {}\n",
        "        for f in all_files:\n",
        "            with open(f, \"r\") as file:\n",
        "                data = json.load(file)\n",
        "            ds[f] = pd.DataFrame(data)\n",
        "        return ds\n",
        "\n"
      ],
      "metadata": {
        "id": "917AsVcjrh4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for deduplication\n",
        "def get_embeddings(texts, batch_size = 64):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc = \"Generating embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        embeddings.extend(classifier.encode(texts = batch_texts)['text_embeddings'])\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def compute_similarity(embeddings, threshold = 0.9):\n",
        "    # n = len(embeddings)\n",
        "    # to_remove = set()\n",
        "    # for i in tqdm(range(n), desc = \"Computing similarity\"):\n",
        "    #     for j in range(i+1, n):\n",
        "    #         sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))[0][0]\n",
        "    #         if sim > threshold:\n",
        "    #             to_remove.add(j)\n",
        "    # return to_remove\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "    np.fill_diagonal(similarity_matrix, 0)  # Ignore self-similarity\n",
        "\n",
        "    # Find indices of pairs with similarity above the threshold\n",
        "    to_remove = set()\n",
        "    for i in range(similarity_matrix.shape[0]):\n",
        "        if i in to_remove:\n",
        "            continue\n",
        "        similar_indices = np.where(similarity_matrix[i] > threshold)[0]\n",
        "        to_remove.update(similar_indices)\n",
        "\n",
        "    return to_remove\n",
        "\n",
        "def compute_similarity_chunked(embeddings, threshold=0.9, chunk_size=8000):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity in chunks to reduce memory usage.\n",
        "    \"\"\"\n",
        "    n = len(embeddings)\n",
        "    to_remove = set()\n",
        "    for i in tqdm(range(0, n, chunk_size), desc= \"Calcuating Similarity\"):\n",
        "        # Get the current chunk\n",
        "        chunk_embeddings = embeddings[i:i + chunk_size]\n",
        "\n",
        "        # Compute cosine similarity for the current chunk against all embeddings\n",
        "        similarity_matrix = cosine_similarity(chunk_embeddings, embeddings)\n",
        "\n",
        "        # Iterate through the chunk rows to find high-similarity indices\n",
        "        for row_idx, similarities in enumerate(similarity_matrix):\n",
        "            actual_idx = i + row_idx  # Map back to the original index\n",
        "            if actual_idx in to_remove:\n",
        "                continue\n",
        "\n",
        "            similar_indices = np.where(similarities > threshold)[0]\n",
        "            similar_indices = [idx for idx in similar_indices if idx > actual_idx]  # Avoid duplicates\n",
        "            to_remove.update(similar_indices)\n",
        "\n",
        "    return to_remove\n",
        "\n",
        "def compute_similarity_between_datasets(embeddings1, embeddings2, threshold = 0.9):\n",
        "    to_remove = set()\n",
        "    for i in tqdm(range(len(embeddings1)), desc = \"Computing similarity\"):\n",
        "        for j in range(len(embeddings2)):\n",
        "            sim = cosine_similarity(embeddings1[i].reshape(1, -1), embeddings2[j].reshape(1, -1))[0][0]\n",
        "            if sim > threshold:\n",
        "                to_remove.add(j)\n",
        "    return to_remove\n",
        "\n",
        "def compute_similarity_between_datasets_chunked(embeddings1, embeddings2, threshold=0.9, chunk_size1=8000, chunk_size2=8000):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two datasets in chunks to reduce memory usage.\n",
        "    Removes entries from embeddings1 based on high similarity with embeddings2.\n",
        "    \"\"\"\n",
        "    to_remove = set()\n",
        "    n1, n2 = len(embeddings1), len(embeddings2)\n",
        "\n",
        "    for i in tqdm(range(0, n1, chunk_size1), desc=\"Processing dataset1 in chunks\"):\n",
        "        # Get a chunk from embeddings1\n",
        "        chunk_embeddings1 = embeddings1[i:i + chunk_size1]\n",
        "\n",
        "        for j in range(0, n2, chunk_size2):\n",
        "            # Get a chunk from embeddings2\n",
        "            chunk_embeddings2 = embeddings2[j:j + chunk_size2]\n",
        "\n",
        "            # Compute cosine similarity for the two chunks\n",
        "            similarity_matrix = cosine_similarity(chunk_embeddings1, chunk_embeddings2)\n",
        "\n",
        "            # Check rows in chunk_embeddings1 with high similarity to chunk_embeddings2\n",
        "            for row_idx, similarities in enumerate(similarity_matrix):\n",
        "                actual_idx = i + row_idx  # Map back to the original index in embeddings1\n",
        "                if actual_idx in to_remove:\n",
        "                    continue\n",
        "                if np.any(similarities > threshold):\n",
        "                    to_remove.add(actual_idx)\n",
        "\n",
        "    return to_remove\n",
        "\n",
        "def deduplication_within_dataset_qa(dataset, threshold = 0.9):\n",
        "    questions = dataset[\"question\"].tolist()\n",
        "    #answers = dataset[\"answer\"].tolist()\n",
        "\n",
        "    question_embeddings = get_embeddings(questions)\n",
        "    to_remove_questions = compute_similarity_chunked(question_embeddings, threshold)\n",
        "\n",
        "    new_dataset = dataset.drop(index = list(to_remove_questions)).reset_index(drop=True)\n",
        "\n",
        "    answers = new_dataset[\"answer\"].tolist()\n",
        "    answer_embeddings = get_embeddings(answers)\n",
        "    to_remove_answers = compute_similarity_chunked(answer_embeddings, threshold)\n",
        "\n",
        "    new_dataset = new_dataset.drop(index = list(to_remove_answers)).reset_index(drop=True)\n",
        "    return new_dataset, list(to_remove_questions), list(to_remove_answers)\n",
        "\n",
        "\n",
        "def deduplicate_across_datasets_qa(new_dataset, old_question_embeddings_saved, old_answer_embeddings_saved, threshold = 0.9):\n",
        "    # Combine all old dataset questions and answers\n",
        "    # all_old_questions = []\n",
        "    # all_old_answers = []\n",
        "\n",
        "    # for dataset in old_datasets:\n",
        "    #     all_old_questions.extend(dataset[\"question\"].tolist())\n",
        "    #     all_old_answers.extend(dataset[\"answer\"].tolist())\n",
        "\n",
        "    # Generate embeddings for old dataset questions and answers\n",
        "    # old_question_embeddings = get_embeddings(all_old_questions)\n",
        "    # old_answer_embeddings = get_embeddings(all_old_answers)\n",
        "    old_question_embeddings = []\n",
        "    old_answer_embeddings = []\n",
        "    for old_embed in old_question_embeddings_saved:\n",
        "        old_question_embeddings.extend(old_embed)\n",
        "    for old_embed in old_answer_embeddings_saved:\n",
        "        old_answer_embeddings.extend(old_embed)\n",
        "\n",
        "    # Generate embeddings for new dataset questions and answers\n",
        "    new_question_embeddings = get_embeddings(new_dataset[\"question\"].tolist())\n",
        "    new_answer_embeddings = get_embeddings(new_dataset[\"answer\"].tolist())\n",
        "\n",
        "    # Deduplicate new questions\n",
        "    to_remove_questions = compute_similarity_between_datasets_chunked(new_question_embeddings, old_question_embeddings)\n",
        "\n",
        "    # Deduplicate new answers\n",
        "    to_remove_answers = compute_similarity_between_datasets_chunked(new_answer_embeddings, old_answer_embeddings)\n",
        "\n",
        "    # Combine removal indices\n",
        "    to_remove = to_remove_questions.union(to_remove_answers)\n",
        "\n",
        "    # Drop duplicates from new dataset\n",
        "    deduplicated_new_dataset = new_dataset.drop(index=list(to_remove)).reset_index(drop=True)\n",
        "\n",
        "    return deduplicated_new_dataset, list(to_remove_questions), list(to_remove_answers)\n",
        "\n"
      ],
      "metadata": {
        "id": "cZdkzAX3rpkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deduplicated data loading\n",
        "deduplicated_medicationqa = pd.read_csv(\"../deduplicated_data/QAs/MedicationQA/medicationqa_train_fulltext_deduplicated.csv\")\n",
        "deduplicated_pubmed1 = pd.read_csv(\"../deduplicated_data/QAs/PubMedQA/ori_pqaa_deduplicated.csv\")\n",
        "deduplicated_pubmed2 = pd.read_csv(\"../deduplicated_data/QAs/PubMedQA/ori_pqau_deduplicated.csv\")\n",
        "deduplicated_pubmed3 = pd.read_csv(\"../deduplicated_data/QAs/PubMedQA/ori_pqal_deduplicated.csv\")"
      ],
      "metadata": {
        "id": "Q_TQDmKfsX_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deduplicate MedMCQA"
      ],
      "metadata": {
        "id": "dh42gZo9tI1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load medmcqa\n",
        "medmcqa = load_dataset(path = directory + \"/MedMCQA\", filetype = \"jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVXdPEkes8pT",
        "outputId": "964840f7-6504-41c9-9754-4ada5d59bb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading JSONL files: 0it [00:00, ?it/s]\n",
            "Processing file: 100%|██████████| 2/2 [00:00<00:00, 22982.49it/s]\n",
            "Loading JSONL files: 1it [00:01,  1.38s/it]\n",
            "Processing file: 100%|██████████| 3/3 [00:00<00:00, 33200.30it/s]\n",
            "Loading JSONL files: 2it [00:01,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current file:  ../dataset/QAs/MedMCQA/data/test.jsonl\n",
            "current file:  ../dataset/QAs/MedMCQA/data/train.jsonl\n",
            "current file:  ../dataset/QAs/MedMCQA/data/dev.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available files\" + str(medmcqa.keys()))\n",
        "medmcqa_train = medmcqa[\"../dataset/QAs/MedMCQA/data/train.jsonl\"]\n",
        "medmcqa_dev = medmcqa[\"../dataset/QAs/MedMCQA/data/dev.jsonl\"]\n",
        "medmcqa_test = medmcqa[\"../dataset/QAs/MedMCQA/data/test.jsonl\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Ti5_xztev1",
        "outputId": "2654f1be-90d4-47a3-d2b3-abeaf9dc1261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available filesdict_keys(['../dataset/QAs/MedMCQA/data/test.jsonl', '../dataset/QAs/MedMCQA/data/train.jsonl', '../dataset/QAs/MedMCQA/data/dev.jsonl'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_medmcqa(df, mode = 'train'):\n",
        "    df['answer'] = None\n",
        "    for i, row in enumerate(df.itertuples()):\n",
        "        if mode != \"test\":\n",
        "            answer_row = f\"The choices are: A) {row.opa}, B) {row.opb}, C) {row.opc}, D) {row.opd}. The correct answer is {row.cop}, because {row.exp}\"\n",
        "        else:\n",
        "            answer_row = f\"The choices are: A) {row.opa}, B) {row.opb}, C) {row.opc}, D) {row.opd}.\"\n",
        "        df.at[i, 'answer'] = answer_row\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "CdI-_uGVte6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medmcqa_train = process_medmcqa(medmcqa_train, mode = 'train')\n",
        "medmcqa_dev = process_medmcqa(medmcqa_dev, mode = 'dev')\n",
        "medmcqa_test = process_medmcqa(medmcqa_test, mode = 'test')"
      ],
      "metadata": {
        "id": "2cDECHPytvsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self deduplication first\n",
        "medmcqa_train_self_dedup, removed_questions_self_train, removed_answers_self_train = deduplication_within_dataset_qa(medmcqa_train)\n",
        "print(len(removed_questions_self_train), len(removed_answers_self_train))\n",
        "medmcqa_dev_self_dedup, removed_questions_self_dev, removed_answers_self_dev = deduplication_within_dataset_qa(medmcqa_dev)\n",
        "print(len(removed_questions_self_dev), len(removed_answers_self_dev))\n",
        "medmcqa_test_self_dedup, removed_questions_self_test, removed_answers_self_test = deduplication_within_dataset_qa(medmcqa_test)\n",
        "print(len(removed_questions_self_test), len(removed_answers_self_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lkK4yYivlNy",
        "outputId": "d0195953-1f02-4c6f-8f7b-038d2b0f4960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 2857/2857 [17:00<00:00,  2.80it/s]\n",
            "Calcuating Similarity: 100%|██████████| 23/23 [02:18<00:00,  6.00s/it]\n",
            "Generating embeddings: 100%|██████████| 2488/2488 [17:34<00:00,  2.36it/s]\n",
            "Calcuating Similarity: 100%|██████████| 20/20 [01:44<00:00,  5.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23598 15601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 66/66 [00:23<00:00,  2.78it/s]\n",
            "Calcuating Similarity: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
            "Generating embeddings: 100%|██████████| 65/65 [00:25<00:00,  2.51it/s]\n",
            "Calcuating Similarity: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 97/97 [00:34<00:00,  2.85it/s]\n",
            "Calcuating Similarity: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
            "Generating embeddings: 100%|██████████| 96/96 [00:34<00:00,  2.79it/s]\n",
            "Calcuating Similarity: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13 674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(medmcqa_train_self_dedup), len(medmcqa_dev_self_dedup), len(medmcqa_test_self_dedup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El3T9TSZvn_-",
        "outputId": "243c7858-a5f8-435f-9061-e6e65a4ef03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143623, 3990, 5463)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, we deduplicate between existing datas"
      ],
      "metadata": {
        "id": "P5yWoJRj-W8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medmcqa_test_self_dedup.to_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_test_fulltext_deduplicated_self.csv\", index = False)\n",
        "medmcqa_dev_self_dedup.to_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_dev_fulltext_deduplicated_self.csv\", index = False)\n",
        "medmcqa_train_self_dedup.to_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_train_fulltext_deduplicated_self.csv\", index = False)"
      ],
      "metadata": {
        "id": "ger-ZMrPMUau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load back data\n",
        "medmcqa_test_self_dedup = pd.read_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_test_fulltext_deduplicated_self.csv\")\n",
        "medmcqa_dev_self_dedup = pd.read_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_dev_fulltext_deduplicated_self.csv\")\n",
        "medmcqa_train_self_dedup = pd.read_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_train_fulltext_deduplicated_self.csv\")"
      ],
      "metadata": {
        "id": "Fr45nNb6GyI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_questions = []\n",
        "old_answers = []\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/medicationqa_question_embeddings.pkl\", \"rb\") as f:\n",
        "    medication_qa_q_embed = pickle.load(f)\n",
        "    old_questions.append(medication_qa_q_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/medicationqa_answer_embeddings.pkl\", \"rb\") as f:\n",
        "    medication_qa_a_embed = pickle.load(f)\n",
        "    old_answers.append(medication_qa_a_embed)\n",
        "\n",
        "#pubmed1,2,3\n",
        "with open(\"../deduplicated_embeddings/QAs/pubmed1_question_embeddings.pkl\", \"rb\") as f:\n",
        "    pubmed1_q_embed = pickle.load(f)\n",
        "    old_questions.append(pubmed1_q_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/pubmed1_answer_embeddings.pkl\", \"rb\") as f:\n",
        "    pubmed1_a_embed = pickle.load(f)\n",
        "    old_answers.append(pubmed1_a_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/pubmed2_question_embeddings.pkl\", \"rb\") as f:\n",
        "    pubmed2_q_embed = pickle.load(f)\n",
        "    old_questions.append(pubmed2_q_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/pubmed2_answer_embeddings.pkl\", \"rb\") as f:\n",
        "    pubmed2_a_embed = pickle.load(f)\n",
        "    old_answers.append(pubmed2_a_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/pubmed3_question_embeddings.pkl\", \"rb\") as f:\n",
        "    pubmed3_q_embed = pickle.load(f)\n",
        "    old_questions.append(pubmed3_q_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/pubmed3_answer_embeddings.pkl\", \"rb\") as f:\n",
        "    pubmed3_a_embed = pickle.load(f)\n",
        "    old_answers.append(pubmed3_a_embed)"
      ],
      "metadata": {
        "id": "87YtNb7yHHLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load already there data\n",
        "full_medmcqa_test_self_dedup, removed_questions_full_test, removed_answers_full_test = deduplicate_across_datasets_qa([deduplicated_medicationqa, deduplicated_pubmed1, deduplicated_pubmed2, deduplicated_pubmed3], medmcqa_test_self_dedup, old_questions, old_answers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlRo1xaU-Smi",
        "outputId": "525c46bb-62a6-49f7-bc10-d01425de9e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 86/86 [00:31<00:00,  2.73it/s]\n",
            "Generating embeddings: 100%|██████████| 86/86 [00:30<00:00,  2.86it/s]\n",
            "Processing dataset1 in chunks: 100%|██████████| 1/1 [00:10<00:00, 10.92s/it]\n",
            "Processing dataset1 in chunks: 100%|██████████| 1/1 [00:10<00:00, 10.39s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(removed_questions_full_test), len(removed_answers_full_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ElyDTNcJOvR",
        "outputId": "f165fefc-5a9b-49c2-fd63-5e4a1b979af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_medmcqa_test_self_dedup.to_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_test_fulltext_deduplicated.csv\", index = False)"
      ],
      "metadata": {
        "id": "f2RrZx1360jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"../deduplicated_embeddings/QAs/medmcqa_test_question_embeddings.pkl\", \"rb\") as f:\n",
        "    medication_qa_q_embed = pickle.load(f)\n",
        "    old_questions.append(medication_qa_q_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/medmcqa_test_answer_embeddings.pkl\", \"rb\") as f:\n",
        "    medication_qa_a_embed = pickle.load(f)\n",
        "    old_answers.append(medication_qa_a_embed)"
      ],
      "metadata": {
        "id": "5LovX8XYKB9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load already there data\n",
        "full_medmcqa_dev_self_dedup, removed_questions_full_dev, removed_answers_full_dev = deduplicate_across_datasets_qa(medmcqa_dev_self_dedup, old_questions, old_answers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVi-lSbs6mfT",
        "outputId": "01891b1d-1ae0-4904-8dd5-f01f04937c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 63/63 [00:22<00:00,  2.81it/s]\n",
            "Generating embeddings: 100%|██████████| 63/63 [00:24<00:00,  2.55it/s]\n",
            "Processing dataset1 in chunks: 100%|██████████| 1/1 [00:09<00:00,  9.08s/it]\n",
            "Processing dataset1 in chunks: 100%|██████████| 1/1 [00:08<00:00,  8.40s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(removed_questions_full_dev), len(removed_answers_full_dev))\n",
        "full_medmcqa_dev_self_dedup.to_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_dev_fulltext_deduplicated.csv\", index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARIMcXDENItX",
        "outputId": "a51fed47-a56f-4523-d555-87741cc6f902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"../deduplicated_embeddings/QAs/medmcqa_dev_question_embeddings.pkl\", \"rb\") as f:\n",
        "    medication_qa_q_embed = pickle.load(f)\n",
        "    old_questions.append(medication_qa_q_embed)\n",
        "\n",
        "with open(\"../deduplicated_embeddings/QAs/medmcqa_dev_answer_embeddings.pkl\", \"rb\") as f:\n",
        "    medication_qa_a_embed = pickle.load(f)\n",
        "    old_answers.append(medication_qa_a_embed)"
      ],
      "metadata": {
        "id": "DUya2gPrNhLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load already there data\n",
        "full_medmcqa_train_self_dedup, removed_questions_full_train, removed_answers_full_train = deduplicate_across_datasets_qa(medmcqa_train_self_dedup, old_questions, old_answers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c4Sb8YN3j0",
        "outputId": "fd951844-d6b4-4eb9-f845-830f3b7d1d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 2245/2245 [13:04<00:00,  2.86it/s]\n",
            "Generating embeddings: 100%|██████████| 2245/2245 [15:48<00:00,  2.37it/s]\n",
            "Processing dataset1 in chunks: 100%|██████████| 18/18 [04:23<00:00, 14.66s/it]\n",
            "Processing dataset1 in chunks: 100%|██████████| 18/18 [04:25<00:00, 14.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(removed_questions_full_train), len(removed_answers_full_train))\n",
        "full_medmcqa_train_self_dedup.to_csv(\"../deduplicated_data/QAs/MedMCQA/medmcqa_train_fulltext_deduplicated.csv\", index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd6NIXxLXf_c",
        "outputId": "89f93341-d41e-4e23-9d7d-ce347013e901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "222 614\n"
          ]
        }
      ]
    }
  ]
}